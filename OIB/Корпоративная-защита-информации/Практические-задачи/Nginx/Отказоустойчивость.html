<!--
title: Отказоустойчивость Nginx (Не закончена)
description: 
published: true
date: 2025-03-11T05:16:39.843Z
tags: 
editor: ckeditor
dateCreated: 2025-02-03T17:24:27.751Z
-->

<h2>Вводная</h2>
<p><strong>Отказоустойчивость в NGINX достигается благодаря функции проверки работоспособности</strong>. Она периодически проверяет работоспособность внутренних серверов и удаляет из пула все не реагирующие на запросы серверы, предотвращая тем самым перебои в работе.</p>
<p>Также для обеспечения отказоустойчивости можно использовать <strong>кластер из как минимум двух серверов</strong>, которые одновременно занимаются задачей распределения запросов/трафика. Такой подход позволяет равномерно распределять запросы между всеми узлами кластера и оптимально использовать ресурсы каждого балансировщика.&nbsp;</p>
<p>Ещё один способ&nbsp;—&nbsp;использовать <strong>пул соединений</strong>. Он позволяет использовать одни и те же соединения для нескольких клиентских запросов, что уменьшает задержку соединения и экономит системные ресурсы.&nbsp;</p>
<p>Также рекомендуется регулярно проводить активные проверки работоспособности, чтобы убедиться в том, что серверы работают нормально. Это поможет обнаружить сбои на ранней стадии и предотвратить отправку запросов на отказавшие серверы.&nbsp;</p>
<h2>Распределение нагрузки на веб-сервисы с помощью прокси-сервера Nginx</h2>
<p style="text-align:justify;"><i>Аннотация:</i> Описывается способ улучшения отказоустойчивости и повышения скорости обработки запросов на веб-сервисы с помощью прокси-сервера Nginx. В статье рассматривается сравнение работы одного и того же веб-сервиса с единственным отличием: первый вариант работает стандартным способом без использования прокси-сервера Nginx, а второй работает с использованием прокси-сервера Nginx.</p>
<p style="text-align:justify;">Веб-сервисы, основанные на архитектуре микросервисов, становятся все более популярными в современном мире. Они позволяют разрабатывать и масштабировать приложения гораздо проще и эффективнее, чем традиционные монолитные системы. Однако, увеличение числа микросервисов также увеличивает нагрузку на сеть и серверы. Для распределения этой нагрузки используются прокси-серверы, одним из которых является Nginx.</p>
<p style="text-align:justify;">Nginx – это высокопроизводительный веб-сервер и прокси-сервер, который широко используется для распределения нагрузки, кеширования, аутентификации и авторизации. Он работает на большом числе операционных систем, включая Linux, Windows, MacOS и FreeBSD. Он также является одним из самых популярных веб-серверов в мире, используемым более чем на 33% всех сайтов в Интернете.</p>
<p style="text-align:justify;">Одним из наиболее распространенных способов использования Nginx является распределение нагрузки на множество серверов, обрабатывающих запросы. Для этого используются конфигурационные файлы, которые определяют параметры прокси-сервера, включающие настройку, отвечающую за распределение нагрузки.</p>
<p style="text-align:justify;">Конфигурационный файл Nginx состоит из блоков, которые могут быть использованы для определения настроек сервера, логирования, кеширования и других параметров. В блоке server можно определить виртуальный сервер, который будет обрабатывать запросы на определенный домен или IP-адрес.</p>
<p style="text-align:justify;">Для распределения нагрузки на несколько серверов в блоке server используется блок upstream. В этом блоке можно указать список серверов и их параметры, такие как IP-адрес, порт и вес. Вес определяет, как часто будет выбран определенный сервер при балансировке нагрузки. Например, если сервер имеет вес 2, то он будет выбираться в два раза чаще, чем сервер с весом 1.</p>
<p style="text-align:justify;">Пример конфигурационного файла Nginx для балансировки нагрузки на два сервера:</p>
<pre><code class="language-plaintext">http {
	upstream myapp {
	server 192.168.1.100:80 weight=2;
	server 192.168.1.101:80;
}

server {
	listen 80;
	
	location / {
		proxy_pass http://myapp;
		}
	}
}</code></pre>
<p style="text-align:justify;">В этом примере мы создали upstream блок с двумя серверами, где первый сервер имеет вес 2, а второй сервер имеет вес по умолчанию 1. Затем мы создали блок server, который прослушивает порт 80 и перенаправляет запросы на upstream блок с помощью директивы proxy_pass.</p>
<p style="text-align:justify;">Nginx также поддерживает различные методы распределения нагрузки, такие как round-robin, least-connected, ip-hash и другие. Метод round-robin выбирает серверы в порядке, определенном в блоке upstream, метод least-connected выбирает сервер с наименьшим количеством соединений, а метод ip-hash выбирает сервер на основе хеша IP-адреса клиента.</p>
<p style="text-align:justify;">Ниже на графике представлено сравнение двух способов работы серверного приложения, которое обрабатывает лишь 1 HTTP-запрос, возвращая всегда статус 200. В первом случае запускается приложение без использования распределения нагрузки. Во втором случае запускается 5 копий одного и того же приложения с использованием прокси-сервера Nginx, который распределяет нагрузки на приложение методов round-robin без указания весов. Результатом сравнения является следующий результат: приложение с использованием прокси-сервера Nginx и распределением нагрузки обрабатывает запросы в среднем в 4 раза быстрее, чем приложение без использования прокси-сервера Nginx и распределения нагрузки.</p>
<p style="text-align:justify;">Веб-сервисы на основе микросервисной архитектуры представляют собой большую нагрузку на серверы и сеть. Для балансировки этой нагрузки используются прокси-серверы, такие как Nginx, которые позволяют распределить нагрузку на несколько серверов, уменьшить нагрузку на один сервер и увеличить отказоустойчивость системы.</p>
<p style="text-align:justify;">Nginx – это высокопроизводительный прокси-сервер, который поддерживает множество протоколов и методов распределения нагрузки. С помощью конфигурационных файлов мы можем определить параметры распределения нагрузки, такие как вес серверов и методы распределения нагрузки.</p>
<p style="text-align:justify;">Использование Nginx для распределения нагрузки на множество серверов помогает улучшить производительность, отказоустойчивость и масштабируемость веб-сервисов на основе микросервисной архитектуры<strong>.</strong></p>
